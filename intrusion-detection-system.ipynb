{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/archive'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install shap imbalanced-learn lightgbm xgboost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-31 20:04:43.794635: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.20.0\n",
      "GPU Available: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-31 20:04:47.403276: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Visulisation Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "#Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score, confusion_matrix,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "#Random Seed \n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"Tensorflow Version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU'))>0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 CSV files:\n",
      "Loading Samples\n"
     ]
    }
   ],
   "source": [
    "base_path = './archive'\n",
    "\n",
    "#all csv files\n",
    "try:\n",
    "    csv_files = [f for f in os.listdir(base_path) if f.endswith('csv')]\n",
    "    print(f\"Found {len(csv_files)} CSV files:\")\n",
    "    for i, file in enumerate(csv_files,1):\n",
    "        print(f\" {i}.{file}\")\n",
    "except:\n",
    "    print(\"The base path for the dataset is not correct.\")\n",
    "    csv_files = []\n",
    "\n",
    "\n",
    "#load all csv files and combine load a subset first\n",
    "dfs = []\n",
    "\n",
    "if csv_files:\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df_temp = pd.read_csv(os.path.join(base_path, file), encoding='utf-8')\n",
    "            dfs.append(df_temp)\n",
    "            print(f\"Loaded {file}: {df_temp.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error Loading..\")\n",
    "\n",
    "    #combine all dataframes\n",
    "    df = pd.concat(dfs, ignore_index = True)\n",
    "    print(f\"Total Dataset: {df.shape}\")\n",
    "    print(f\"Total Rows: {df.shape[0]:,} | Columns: {df.shape[1]}\")\n",
    "\n",
    "else:\n",
    "    print(\"Loading Samples\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "\n",
    "#display first few rows\n",
    "if not df.empty:\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Dimensions: 0 rows x 0 columns\n",
      "Memory Usage: 0.00 MB\n",
      "Column Data Types:\n",
      "Series([], Name: count, dtype: int64)\n",
      "Label Column not Found !!\n",
      "\n",
      " Missing values:\n",
      "Empty DataFrame\n",
      "Columns: [Missing_Count, Percentage]\n",
      "Index: []\n",
      "Statistical Summary of first 5 columns:\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset Dimensions: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "print(f\"Memory Usage: {df.memory_usage(deep=True).sum()/1024**2:.2f} MB\")\n",
    "\n",
    "#Column names and datatypes\n",
    "print(f\"Column Data Types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "#Label or  Label Column\n",
    "label_col = 'Label' if 'Label' in df.columns else ' Label' if ' Label' in df.columns else None\n",
    "\n",
    "if label_col:\n",
    "    print(f\"\\nTarget Variable: '{label_col}'\")\n",
    "    print(f\"\\nAttack type Distribution:\")\n",
    "    attack_counts = df[label_col].value_counts()\n",
    "    print(attack_counts)\n",
    "\n",
    "    print(f\"\\n Percentage Distribution:\")\n",
    "    print((df[label_col].value_counts(normalize=True)*100).round(2))\n",
    "else:\n",
    "    print(\"Label Column not Found !!\")\n",
    "\n",
    "\n",
    "#Missing values\n",
    "print(\"\\n Missing values:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing/len(df))*100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "}).sort_values(by = 'Missing_Count', ascending=False).head(10)\n",
    "print(missing_df[missing_df['Missing_Count']>0])\n",
    "\n",
    "#Statistical Summary of first 5 numeric columns\n",
    "print(f\"Statistical Summary of first 5 columns:\")\n",
    "#display(df.describe().iloc[:, :5].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA analysis of Attack Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if label_col and not df.empty:\n",
    "    fig, axes = plt.subplots(2,2, figsize=(24,18))\n",
    "\n",
    "    #Attack Type Count bar plot\n",
    "    attack_counts = df[label_col].value_counts()\n",
    "    ax1 = axes[0,0]\n",
    "    attack_counts.plot(kind='bar', ax=ax1, color='steelblue', edgecolor = 'black')\n",
    "    ax1.set_title('Attack Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Attack Type', fontsize =12)\n",
    "    ax1.set_ylabel('Count', fontsize =12)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(axis = 'y', alpha=0.3)\n",
    "\n",
    "    #add count labels on bars\n",
    "    for i,v in enumerate(attack_counts.values):\n",
    "        ax1.text(i, v, f'{v:,}', ha ='center', va='bottom', fontsize=9)\n",
    "\n",
    "    #Attack Type Percentage(Pie Chart- Top Attacks)\n",
    "    ax2 = axes[0,1]\n",
    "    top_attacks = attack_counts.head(8)\n",
    "    colors = plt.cm.Set3(range(len(top_attacks)))\n",
    "    ax2.pie(top_attacks.values, labels = top_attacks.index, autopct='%1.1f%%',\n",
    "           startangle = 90, colors=colors)\n",
    "    ax2.set_title('Top 8 attacks types(percentage):', fontsize=16, fontweight='bold')\n",
    "\n",
    "    #Log Scale Distribution\n",
    "    ax3 = axes[1,0]\n",
    "    attack_counts.plot(kind='bar', ax=ax3, color='coral', edgecolor='black', log = True)\n",
    "    ax3.set_title('Attack Distribution Log Scale', fontsize = 14, fontweight='bold')\n",
    "    ax3.set_xlabel('Attack count', fontsize =12)\n",
    "    ax3.set_ylabel('Count (Log Scale)', fontsize=12)\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    #Binary Classification (benign vs Malicious)\n",
    "    ax4 = axes[1,1]\n",
    "    binary_dist = df[label_col].apply(lambda x: 'BENIGN' if x== 'BENIGN' else 'MALICIOUS').value_counts()\n",
    "    colors_binary = ['#2ecc71', '#e74c3c']\n",
    "    ax4.bar(binary_dist.index, binary_dist.values, color=colors_binary, edgecolor='black', width=0.6)\n",
    "    ax4.set_title('Binary Classification(BENIGN vs MALICIOUS):', fontsize=14, fontweight='bold')\n",
    "    ax4.set_ylabel('Count', fontsize=12)\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    #Add count labels\n",
    "    for i,v in enumerate(binary_dist.values):\n",
    "        ax4.text(i, v, f\"{v:,}\\n({v/len(df)*100:.1f}%)\", ha='center', va='bottom', fontsize =11, fontweight = 'bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    #Key Statistics\n",
    "    print('Key Statistics')\n",
    "    print(f\"Total Samples: {len(df):,}\")\n",
    "    print(f\"Benign Traffic: {(df[label_col]=='BENIGN').sum():,} ({(df[label_col]=='BENIGN').sum()/len(df)*100:.2f}%)\")\n",
    "    print(f\"Malicious Traffic: {(df[label_col] != 'BENIGN').sum():,} ({(df[label_col] != 'BENIGN').sum()/len(df)*100:.2f}%)\")\n",
    "    print(f\"Number of Attack Types: {df[label_col].nunique()}\")\n",
    "    print(f\"Imbalance Ratio: 1:{len(df)//attack_counts.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    #select only numeric features only\n",
    "    numeric_cols = [\n",
    "        c for c in df.select_dtypes(include=[np.number]).columns\n",
    "        if c != 'is_attack'\n",
    "    ]\n",
    "    # numeric_cols = [c for c in numeric_cols if c != 'is_attack']\n",
    "\n",
    "    #remove infinite values\n",
    "    df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    print(f\"Total Numeric Features:{len(numeric_cols)}\")\n",
    "\n",
    "    #Feature Statistics\n",
    "    print(\"Features with zero variance to be removed:\")\n",
    "    zero_var_features = df[numeric_cols].std()[df[numeric_cols].std()==0].index.tolist()\n",
    "    print(f\"Found {len(zero_var_features)} zero-variance features\")\n",
    "    if zero_var_features:\n",
    "        print(f\" {zero_var_features[:10]}\")\n",
    "\n",
    "\n",
    "    #correlation with target binary classification\n",
    "    if label_col:\n",
    "        print(f\"creating binary target for binary classification\")\n",
    "        df['is_attack'] = (df[label_col] != 'BENIGN').astype(int)\n",
    "\n",
    "        #calculate correlations\n",
    "        correlations = df[numeric_cols + ['is_attack']].corr().loc[numeric_cols,'is_attack'].abs().sort_values(ascending=False)\n",
    "\n",
    "        #plot top correlated features\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16,5))\n",
    "\n",
    "        #Top 15 correlated features\n",
    "        ax1 = axes[0]\n",
    "        top_corr = correlations.head(15)\n",
    "        top_corr.plot(kind='barh', ax=ax1, color = 'teal', edgecolor='black')\n",
    "        ax1.set_title('Top 15 features Coorelated with Attacks:', fontsize = 13, fontweight='bold')\n",
    "        ax1.set_xlabel('Absolute Correlation' , fontsize=11)\n",
    "        ax1.invert_yaxis()\n",
    "        ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "        ax2 = axes[1]\n",
    "        ax2.hist(correlations.values, bins=30, color='skyblue', edgecolor='black')\n",
    "        ax2.set_title('Distribution of Feature Correlation', fontsize=13, fontweight='bold')\n",
    "        ax2.set_xlabel('Absolute Correaltion with Attack', fontsize=11)\n",
    "        ax2.set_ylabel('Frequency', fontsize=11)\n",
    "        ax2.axvline(correlations.median(), color='red', linestyle='--', linewidth = 2, label=f\"Median: {correlations.median():.3f}\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\n Top 10 important features by correlation\")\n",
    "        print(correlations.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and len(numeric_cols)>0:\n",
    "    if label_col and 'is_attack' in df.columns:\n",
    "        top_features = correlations.head(20).index.tolist()\n",
    "\n",
    "        corr_matrix = df[top_features].corr()\n",
    "\n",
    "        #plot heatmap\n",
    "        plt.figure(figsize=(14,12))\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool)) #mask upper triangle\n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm',\n",
    "                   center=0, square=True, linewidths=0.5, cbar_kws={\"shrink\":0.8})\n",
    "        plt.title('Coorelation heatmap for top 20 Attack Related features:',\n",
    "                 fontsize=14, fontweight='bold',pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        #highly correlated feature pairs potential multicollinearity\n",
    "        print(\"Highly Correlated Feature Pairs(>0.9)\")\n",
    "        high_corr_pairs = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                if abs(corr_matrix.iloc[i,j])>0.9:\n",
    "                    high_corr_pairs.append((corr_matrix.columns[i],\n",
    "                                           corr_matrix.columns[j],\n",
    "                                           corr_matrix.iloc[i,j]))\n",
    "\n",
    "        if high_corr_pairs:\n",
    "            for feat1, feat2, corr_val in high_corr_pairs[:10]:\n",
    "                print(f\" .{feat1}<->{feat2}: {corr_val:.3f}\")\n",
    "        else:\n",
    "            print(f\" No highly correlated featured pairs found good for model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing [] numeric columns..\n",
      " Found 0.0 infinite values\n",
      " Before Missing Values: 0.0\n",
      " Found 0 Zero Variation Features.\n",
      "\n",
      " Duplicate Rows: 0\n",
      "Memory Reduced: 0.00MB -> 0.00MB\n",
      " Reduced by 0.0%\n",
      "Data Cleaning Done.\n",
      "Final Shape : (0, 0)\n"
     ]
    }
   ],
   "source": [
    "df_clean = df.copy()\n",
    "\n",
    "#clean column names remove trailing/leading spaces\n",
    "#df_clean.columns = df_clean.columns.str.strip()\n",
    "label_col = 'Label' if 'Label' in df_clean.columns else label_col\n",
    "\n",
    "#handling infinite values replaced with nan\n",
    "#btw already remove in previous cell\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Preprocessing {numeric_cols} numeric columns..\")\n",
    "\n",
    "inf_count = np.isinf(df_clean[numeric_cols]).sum().sum()\n",
    "print(f\" Found {inf_count:,} infinite values\")\n",
    "\n",
    "if inf_count>0:\n",
    "    df_clean[numeric_cols] = df_clean[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "#handling missing values with median of numeric columns\n",
    "missing_before = df_clean.isnull().sum().sum()\n",
    "print(f\" Before Missing Values: {missing_before:,}\")\n",
    "\n",
    "if missing_before>0:\n",
    "    for col in numeric_cols:\n",
    "        if df_clean[col].isnull().sum()>0:\n",
    "            df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
    "\n",
    "    missing_after = df_clean.isnull().sum().sum()\n",
    "    print(f\" After imputation {missing_after:,} missing values\")\n",
    "\n",
    "#remvoing zero variance features\n",
    "variance = df_clean[numeric_cols].var()\n",
    "zero_var_cols = variance[variance==0].index.tolist()\n",
    "\n",
    "print(f\" Found {len(zero_var_cols)} Zero Variation Features.\")\n",
    "if zero_var_cols:\n",
    "    df_clean = df_clean.drop(columns=zero_var_cols)\n",
    "    print(f\"Removed Zero Variance Features.\")\n",
    "    numeric_cols = [col for col in numeric_cols if col not in zero_var_cols]\n",
    "\n",
    "#Removing duplicate Rows\n",
    "duplicates = df_clean.duplicated().sum()\n",
    "print(f\"\\n Duplicate Rows: {duplicates}\")\n",
    "if duplicates>0:\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    print(f\"Removed {duplicates:,} Duplicate Rows\")\n",
    "\n",
    "#Data Type Optimisation (reduce memory usage)\n",
    "memory_before = df_clean.memory_usage(deep=True).sum()/1024**2\n",
    "\n",
    "for col in numeric_cols:\n",
    "    col_min = df_clean[col].min()\n",
    "    col_max = df_clean[col].max()\n",
    "\n",
    "    #integer optimisation\n",
    "    if df_clean[col].dtype=='int64':\n",
    "        if col_min>=0 and col_max<=255:\n",
    "            df_clean[col] = df_clean[col].astype('uint8')\n",
    "        elif col_min>=0 and col_max<=65535:\n",
    "            df_clean[col] = df_clean[col].astype('uint16')\n",
    "        elif col_min>=-32768 and col_max<=32767:\n",
    "            df_clean[col] = df_clean[col].astype('int16')\n",
    "        elif col_min>= -2147483648 and col_max<= 2147483647:\n",
    "            df_clean[col] = df_clean[col].astype('int32')\n",
    "\n",
    "    #float optimisation\n",
    "    elif df_clean[col].dtype=='float64':\n",
    "        df_clean[col]=df_clean[col].astype('float32')\n",
    "\n",
    "memory_after = df_clean.memory_usage(deep=True).sum()/ 1024**2\n",
    "print(f\"Memory Reduced: {memory_before:.2f}MB -> {memory_after:.2f}MB\")\n",
    "print(f\" Reduced by {(1- memory_after/memory_before)*100:.1f}%\")\n",
    "\n",
    "print(\"Data Cleaning Done.\")\n",
    "print(f\"Final Shape : {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Column not founs!!\n"
     ]
    }
   ],
   "source": [
    "if label_col in df_clean.columns:\n",
    "    #separate feature and target\n",
    "    X = df_clean.drop(columns=[label_col, 'is_attack'], errors='ignore')\n",
    "    y = df_clean[label_col].copy()\n",
    "\n",
    "    print(f\"Origninal Label Distributions:\")\n",
    "    print(y.value_counts())\n",
    "\n",
    "    #Multi Class Encoding\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    #create binary target for binary classification pipeline\n",
    "    y_binary = (y != 'BENGIN').astype(int)\n",
    "\n",
    "    print(\"Label Encoding Complete\")\n",
    "    print(f\"Classes: {len(le.classes_)}\")\n",
    "    print(f\"Encoding Labels: {np.unique(y_encoded)}\")\n",
    "\n",
    "    #display mapping \n",
    "    label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    for original,encoded in sorted(label_mapping.items(), key=lambda x: x[1]):\n",
    "        count = (y==original).sum()\n",
    "        print(f\" {encoded:2d} <- {original:30s} (n={count:,})\")\n",
    "\n",
    "    #feature selection remove non numeric columns\n",
    "    X_numeric = X.select_dtypes(include=[np.number])\n",
    "\n",
    "    print(\"Featue Matrix Ready:\")\n",
    "    print(f\"Shape: {X_numeric.shape}\")\n",
    "    print(f\"Features: {X_numeric.shape[1]}\")\n",
    "    print(f\"Samples: {X_numeric.shape[0]:,}\")\n",
    "\n",
    "    \n",
    "    #store feature names\n",
    "    feature_names = X_numeric.columns.tolist()\n",
    "else:\n",
    "    print(\"Label Column not founs!!\")\n",
    "    X_numeric, y_encoded, y_binary, feature_names = None, None, None, []\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot perform Split-data not ready!!\n"
     ]
    }
   ],
   "source": [
    "if X_numeric is not None and y_encoded is not None:\n",
    "    #stratified split to maintain class distribution\n",
    "    X_train,X_test, y_train, y_test = train_test_split(\n",
    "        X_numeric, y_encoded,\n",
    "        test_size = 0.3,\n",
    "        random_state=42,\n",
    "        stratify = y_encoded\n",
    "    )\n",
    "\n",
    "    #Binary Version\n",
    "    _,_,y_train_binary, y_test_binary = train_test_split(\n",
    "        X_numeric, y_binary,\n",
    "        test_size =0.3,\n",
    "        random_state = 42,\n",
    "        stratify=y_binary\n",
    "    )\n",
    "\n",
    "    print(\"Split Summary\")\n",
    "    print(f\" Train Set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X_numeric)*100:.1f}%)\")\n",
    "    print(f\" Test Set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X_numeric)*100:.1f}%)\")\n",
    "    print(f\" Features: {X_train.shape[1]}\")\n",
    "\n",
    "    print(f\"\\n Class Distribution in Training Set:\")\n",
    "    train_dist = pd.Series(y_train).value_counts().sort_index()\n",
    "    for class_id, count in train_dist.items():\n",
    "        class_name = le.inverse_transform([class_id])[0]\n",
    "        print(f\" Class {class_id:2d} ({class_name:30s}): {count:7,} ({count/len(y_train)*100:5.2f}%)\")\n",
    "\n",
    "    print(\"Train Test Split Completed\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot perform Split-data not ready!!\")\n",
    "    X_train, X_test, y_train, y_test = None, None, None, None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot Scale: Training Data not Available!!!\n"
     ]
    }
   ],
   "source": [
    "if X_train is not None:\n",
    "    #standardscaler: mean=0, std=1\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    print(\"Fitting scaler on training data..\")\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "    #convert back to dataframes\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns = feature_names)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns = feature_names)\n",
    "\n",
    "    #verify scaling\n",
    "    print(\"Scaling Verification for first 5 features:\")\n",
    "    print(\" Original Training Data:\")\n",
    "    print(X_train.iloc[:,:5].describe().loc[['mean','std']])\n",
    "\n",
    "    print(\"Scaled Training Data:\")\n",
    "    print(X_train_scaled.iloc[:,:5].describe().loc[['mean','std']])\n",
    "\n",
    "    print(\"Feature scaling completed!!\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot Scale: Training Data not Available!!!\")\n",
    "    X_train_scaled , X_test_Scaled = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  5. Handling Class Imbalance\n",
    "\n",
    "**Critical for Cybersecurity:**\n",
    "Class imbalance is a major challenge in intrusion detection. Some attacks are rare but critical.\n",
    "\n",
    "**Techniques we'll use:**\n",
    "1. **SMOTE** (Synthetic Minority Over-sampling Technique) - Generate synthetic samples\n",
    "2. **Random Under-sampling** - Reduce majority class\n",
    "3. **Class weights** - Penalize misclassification of minority classes\n",
    "\n",
    "**Why this matters:**\n",
    "- Rare attacks (e.g., Infiltration) must still be detected\n",
    "- Models trained on imbalanced data tend to favor majority class\n",
    "- In security, missing a rare but dangerous attack is unacceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munder_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomUnderSampler\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pipeline \u001b[38;5;28;01mas\u001b[39;00m ImbPipeline\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train_scaled is not None and y_train is not None:\n",
    "    print(\"Original Class Distribution:\")\n",
    "    original_dist = Counter(y_train)\n",
    "    for class_id, count in sorted(original_dist.items()):\n",
    "        class_name = le.inverse_transform([class_id])[0]\n",
    "        print(f\" Class {class_id:2d} ({class_name:30s}): {count:7,}\")\n",
    "\n",
    "    print(\"Applying SMOTE + Random Under Sampling...\")\n",
    "\n",
    "    #over-sample minority classes to 10% of majority\n",
    "    #then under-sample majority to reduce size\n",
    "\n",
    "    #calculate target-samples\n",
    "    majority_class = max(original_dist, key=original_dist.get)\n",
    "    majority_count = original_dist[majority_class]\n",
    "    target_minority = int(majority_count*0.1) #10% of majority\n",
    "\n",
    "    #SMOTE over-sampling minorities\n",
    "    sampling_strategy_over = {\n",
    "        cls: max(count, target_minority)\n",
    "        for cls, count in original_dist.items()\n",
    "        if cls != majority_class and count<target_minority\n",
    "    }\n",
    "\n",
    "    #under sampling majority\n",
    "    target_majority = int(target_minority*5)\n",
    "    sampling_strategy_under = {majority_class: target_majority}\n",
    "\n",
    "    #create pipeline\n",
    "    over = SMOTE(sampling_strategy=sampling_strategy_over, random_state=42)\n",
    "    under = RandomUnderSampler(sampling_strategy=sampling_strategy_under, random_state=42)\n",
    "\n",
    "    pipeline = ImbPipeline(steps=[('over',over), ('under', under)])\n",
    "\n",
    "    #apply resampling\n",
    "    X_train_resampled , y_train_resampled = pipeline.fit_resample(X_train_scaled,y_train)\n",
    "\n",
    "    print(\"Resampling Completed..\")\n",
    "    print(\"New Class Distribution..\")\n",
    "    new_dist = Counter(y_train_resampled)\n",
    "    for class_id, count in sorted(new_dist.items()):\n",
    "        class_name = le.inverse_transform([class_id])[0]\n",
    "        change = count - original_dist.get(class_id,0)\n",
    "        print(f\"Clas:{class_id:2d} ({class_name:30s}): {count:7,} ({change:+7,})\")\n",
    "\n",
    "    print(\"Dataset Size:\")\n",
    "    print(f\" Before: {len(y_train):,} samples\")\n",
    "    print(f\" After: {len(y_train_resampled):,} samples\")\n",
    "    print(f\" Changed: {len(y_train_resampled)- len(y_train):+,} samples\")\n",
    "\n",
    "    #visualisation\n",
    "    fig, axes = plt.subplots(1,2, figsize = (16,5))\n",
    "\n",
    "    #Before reSampling\n",
    "    ax1 = axes[0]\n",
    "    classes_before = sorted(original_dist.keys())\n",
    "    counts_before = [original_dist[c] for c in classes_before]\n",
    "    ax1.bar(classes_before, counts_before, color='coral', edgecolor = 'black')\n",
    "    ax1.set_title(\"Class Distribution before Sampling\", fontsize =13, fontweight = 'bold')\n",
    "    ax1.set_xlabel('Class ID', fontsize=11)\n",
    "    ax1.set_ylabel('count', fontsize=11)\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    #After reSampling\n",
    "    ax2 = axes[1]\n",
    "    classes_after = sorted(new_dist.keys())\n",
    "    counts_after = [new_dist[c] for c in classes_after]\n",
    "    ax2.bar(classes_after, counts_after, color='coral', edgecolor='black')\n",
    "    ax2.set_title(\"Class Distribution after Sampling\", fontsize=13, fontweight = 'bold')\n",
    "    ax2.set_xlabel('Class Id', fontsize=11)\n",
    "    ax2.set_ylabel('count', fontsize=11)\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Cannot apply resampling, data not ready!!!\")\n",
    "    X_train_resampled, y_train_resampled = X_train_scaled, y_train\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models:\n",
    "1. **Logistic Regression** - Linear baseline\n",
    "2. **Random Forest** - Ensemble method, handles non-linearity\n",
    "3. **XGBoost** - Gradient boosting, excellent for tabular data\n",
    "4. **LightGBM** - Fast gradient boosting\n",
    "\n",
    "### Why Tree-based Models Excel in Cybersecurity:\n",
    "-  **Handle non-linear relationships** (network patterns are complex)\n",
    "-  **Robust to outliers** (anomalous traffic patterns)\n",
    "-  **Feature interactions** (attacks often involve multiple feature combinations)\n",
    "-  **No feature scaling required** (though we still normalize)\n",
    "-  **Interpretable** (feature importance for security analysts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing with GPU accelrated library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.linear_model import LogisticRegression as cuLR\n",
    "from cuml.ensemble import RandomForestClassifier as cuRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train_resampled is not None:\n",
    "    print(\"Training Logistic Regression:\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    #for multi-class , use mutlinomial saga solver\n",
    "    # lr_model = LogisticRegression(\n",
    "    #     max_iter = 1000,\n",
    "    #     multi_class = 'multinomial',\n",
    "    #     solver = 'saga',\n",
    "    #     random_state = 42,\n",
    "    #     n_jobs = -1,\n",
    "    #     verbose = 0\n",
    "    # )\n",
    "    lr_model = cuLR(max_iter=2000, penalty='l2', solver='qn')\n",
    "\n",
    "    lr_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    train_time = time.time()-start_time\n",
    "\n",
    "    #predictions\n",
    "    y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "    y_pred_proba_lr = lr_model.predict_proba(X_test_scaled)\n",
    "\n",
    "    #Evaluaton Metrics\n",
    "    accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "    precision_lr = precision_score(y_test, y_pred_lr, average='weighted', zero_division=0)\n",
    "    recall_lr = recall_score(y_test, y_pred_lr, average='weighted', zero_division=0)\n",
    "    f1_lr = f1_score(y_test, y_pred_lr, average='weighted', zero_division=0)\n",
    "\n",
    "    print(\"Training Logistic Regression Complete:\")\n",
    "    print(f\"Training Time: {train_time:.2f} seconds\")\n",
    "    print(f\"Performance Metrics: \")\n",
    "    print(f\"Accuracy: {accuracy_lr:.4f}\")\n",
    "    print(f\"Precision: {precision_lr:.4f}\")\n",
    "    print(f\"Recall: {recall_lr:.4f}\")\n",
    "    print(f\"f1 score: {f1_lr:.4f}\")\n",
    "\n",
    "    #detailed Classification Report\n",
    "    print(\"Classification Report for Logistic Regression\")\n",
    "    print(classification_report(\n",
    "        y_test, y_pred_lr, target_names = le.classes_,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "else:\n",
    "    print(\"Cannot train the LR model, training data not ready !!!\")\n",
    "    lr_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train_resampled is not None:\n",
    "    print(\"Training Random Forest Model:\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # rf_model = RandomForestClassifier(\n",
    "    #     n_estimators = 100,\n",
    "    #     max_depth = 20,\n",
    "    #     min_samples_split = 10,\n",
    "    #     min_samples_leaf = 10,\n",
    "    #     random_state = 42,\n",
    "    #     n_jobs = -1,\n",
    "    #     verbose = 0,\n",
    "    #     class_weight = 'balanced'\n",
    "    # )\n",
    "    rf_model = cuRF(n_estimators=100, max_depth = 10, random_state=42)\n",
    "\n",
    "    rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    train_time = time.time()-start_time\n",
    "\n",
    "    #predictions\n",
    "    y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "    y_pred_proba_rf = rf_model.predict_proba(X_test_scaled)\n",
    "\n",
    "    #Metrics \n",
    "    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "    precision_rf = precision_score(y_test, y_pred_rf, average = 'weighted', zero_division=0)\n",
    "    recall_rf = recall_score(y_test, y_pred_rf, average='weighted', zero_division=0)\n",
    "    f1_rf = f1_score(y_test, y_pred_rf, average='weighted', zero_division=0)\n",
    "\n",
    "    print(\"Training Random Forest Complete:\")\n",
    "    print(f\"Training Time: {train_time:.2f} seconds\")\n",
    "    print(\"Evaluation Metrics for Training:\")\n",
    "    print(f\" Accuracy Score: {accuracy_rf:.4f}\")\n",
    "    print(f\" Precision Score: {precision_rf:.4f}\")\n",
    "    print(f\" Recall Score: {recall_rf:.4f}\")\n",
    "    print(f\" f1 Score: {f1_rf:.4f}\")\n",
    "\n",
    "    #detailed classification report\n",
    "    print(\"Detailed Classification Report:\")\n",
    "    print(classification_report(\n",
    "        y_test, y_pred_rf,\n",
    "        target_names = le.classes_,\n",
    "        zero_division = 0\n",
    "    ))\n",
    "\n",
    "else:\n",
    "    print(\"Cannot Train Random Forest model , no training data.\")\n",
    "    rf_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train_resampled is not None:\n",
    "    print(\"Training XGBoost Model:\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    #calculate scale_pos_weight for class_imbalance\n",
    "    class_counts = Counter(y_train_resampled)\n",
    "    n_classes = len(class_counts)\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators = 100,\n",
    "        max_depth = 10,\n",
    "        learning_rate = 0.1,\n",
    "        subsample = 0.8,\n",
    "        colsample_bytree = 0.8,\n",
    "        random_state = 42,\n",
    "        n_jobs = -1,\n",
    "        tree_method = \"hist\",\n",
    "        predictor = \"cpu_predictor\",\n",
    "        objective = 'multi:softmax' if n_classes>2 else 'binary:logistic',\n",
    "        num_class = n_classes if n_classes>2 else None,\n",
    "        eval_metric = 'mlogloss' if n_classes>2 else 'logloss' \n",
    "    )\n",
    "\n",
    "    xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "    train_time = time.time()-start_time\n",
    "\n",
    "    #predictions \n",
    "    y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "    y_pred_xgb_prob = xgb_model.predict_proba(X_test_scaled)\n",
    "\n",
    "    #evaluation metrics\n",
    "    accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "    precision_xgb = precision_score(y_test, y_pred_xgb, average='weighted', zero_division=0)\n",
    "    recall_xgb = recall_score(y_test, y_pred_xgb, average='weighted', zero_division=0)\n",
    "    f1_xgb = f1_score(y_test, y_pred_xgb, average='weighted', zero_division=0)\n",
    "\n",
    "    print(\"Training for XGBoost Complete:\")\n",
    "    print(f\"Training Time: {train_time:.2f} seconds\")\n",
    "    print(f\" Accuracy Score: {accuracy_xgb:.4f}\")\n",
    "    print(f\" Precision Score: {precision_xgb:.4f}\")\n",
    "    print(f\" Recall Score: {recall_xgb:.4f}\")\n",
    "    print(f\" f1 Score: {f1_xgb:.4f}\")\n",
    "\n",
    "    #classification report for XGBoost training\n",
    "    print(\"Detailed Classification Report:\")\n",
    "    print(classification_report(\n",
    "        y_test, y_pred_xgb,\n",
    "        target_names = le.classes_,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "else:\n",
    "    print(\"Cannot Train XGBoost model not training data.\")\n",
    "    xgb_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_resampled.shape)\n",
    "print(np.unique(y_train_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train_resampled is not None:\n",
    "    print(\"Training LightGBM model:\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators = 100,\n",
    "        max_depth = 10,\n",
    "        learning_rate = 0.1,\n",
    "        num_leaves = 31,\n",
    "        subsample = 0.8,\n",
    "        colsample_bytree = 0.8,\n",
    "        random_state = 42,\n",
    "        n_jobs = -1,\n",
    "        device = \"cpu\",\n",
    "        verbose = -1,\n",
    "        class_weight = 'balanced'\n",
    "    )\n",
    "\n",
    "    lgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "    train_time = time.time()-start_time\n",
    "\n",
    "    #predictions\n",
    "    y_pred_lgb = lgb_model.predict(X_test_scaled)\n",
    "    y_pred_lgb_proba = lgb_model.predict_proba(X_test_scaled)\n",
    "\n",
    "    #evaluation metrics\n",
    "    accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
    "    precision_lgb = precision_score(y_test, y_pred_lgb, average='weighted', zero_division=0)\n",
    "    recall_lgb = recall_score(y_test, y_pred_lgb, average='weighted', zero_division=0)\n",
    "    f1_lgb = f1_score(y_test, y_pred_lgb, average='weighted', zero_division=0)\n",
    "\n",
    "    print(\"Training for LightGBM complete:\")\n",
    "    print(f\"Training Time: {train_time:.2f}\")\n",
    "    print(f\" Accuracy Score: {accuracy_lgb:.4f}\")\n",
    "    print(f\" Precision Score: {precision_lgb:.4f}\")\n",
    "    print(f\" f1 Score: {f1_lgb:.4f}\")\n",
    "\n",
    "    #detailed classification report for LightGBM model\n",
    "    print(\"Classification Report for LightGBM Model:\")\n",
    "    print(classification_report(\n",
    "        y_test, y_pred_lgb, \n",
    "        target_names = le.classes_,\n",
    "        zero_division = 0\n",
    "    ))\n",
    "\n",
    "else:\n",
    "    print(\"Cannot train LightGBM model as not training data available !!!\")\n",
    "    lgb_model = None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compairing ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create comparison DataFrame\n",
    "ml_results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost', 'LightGBM'],\n",
    "    'Accuracy': [accuracy_lr, accuracy_rf, accuracy_xgb, accuracy_lgb],\n",
    "    'Precision': [precision_lr, precision_rf, precision_xgb, precision_lgb],\n",
    "    'Recall': [recall_lr, recall_rf, recall_xgb, recall_lgb],\n",
    "    'F1-Score': [f1_lr, f1_rf, f1_xgb, f1_lgb]\n",
    "})\n",
    "\n",
    "print(\"Performance Summary:\")\n",
    "print(ml_results.to_string(index=False))\n",
    "\n",
    "#finind best model\n",
    "best_model_idx = ml_results['F1-Score'].idxmax()\n",
    "best_model_name = ml_results.loc[best_model_idx, 'Model']\n",
    "print(f\" Best ML Model: {best_model_name}\")\n",
    "\n",
    "#visualisation\n",
    "fig, axes = plt.subplots(2,2,figsize=(16,10))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen', 'plum']\n",
    "\n",
    "for idx,(metric,color) in enumerate(zip(metrics,colors)):\n",
    "    ax = axes[idx//2, idx%2]\n",
    "    bars = ax.bar(ml_results['Model'], ml_results[metric], color = color, edgecolor = 'black', alpha = 0.8)\n",
    "    ax.set_title(f\"{metric} Comparison\", fontsize = 13, fontweight = 'bold')\n",
    "    ax.set_ylabel(metric, fontsize=11)\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.grid(axis = 'y', alpha = 0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    #add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.4f}', ha='center', va='bottom', fontsize = 10, fontweight='bold')\n",
    "\n",
    "    #highlight best model\n",
    "    best_idx = ml_results[metric].idxmax()\n",
    "    bars[best_idx].set_edgecolor('red')\n",
    "    bars[best_idx].set_linewidth(3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning can capture complex patterns in network traffic that traditional ML models might miss.\n",
    "\n",
    "#### Architecture: Multi-Layer Perceptron (MLP)\n",
    "- **Input Layer**: Network traffic features\n",
    "- **Hidden Layers**: 3-4 dense layers with dropout for regularization\n",
    "- **Output Layer**: Softmax for multi-class classification\n",
    "\n",
    "#### Why Neural Networks for IDS:\n",
    "-  **Learn hierarchical features** automatically\n",
    "-  **Handle high-dimensional data** effectively\n",
    "-  **Capture non-linear patterns** in attack behaviors\n",
    "-  **Adaptable** to new attack types with retraining\n",
    "\n",
    "#### Techniques:\n",
    "- **Batch Normalization**: Stabilize training\n",
    "- **Dropout**: Prevent overfitting\n",
    "- **Early Stopping**: Stop when validation performance plateaus\n",
    "- **Learning Rate Scheduling**: Adaptive learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train_resampled is not None:\n",
    "\n",
    "    #prepate data for neural network\n",
    "    n_features = X_train_resampled.shape[1]\n",
    "    n_classes = len(np.unique(y_train_resampled))\n",
    "\n",
    "    #convert labels into categorical for neural network\n",
    "    y_train_cat = to_categorical(y_train_resampled, num_classes = n_classes)\n",
    "    y_test_cat = to_categorical(y_test, num_classes = n_classes)\n",
    "\n",
    "    print(\"Network Configuration:\")\n",
    "    print(f\" Input Features: {n_features}\")\n",
    "    print(f\" Output Features: {n_classes}\")\n",
    "    print(f\" Training Samples: {len(X_train_resampled):,}\")\n",
    "\n",
    "    #Build neural network\n",
    "    print(\"Building Model Architecture...\")\n",
    "\n",
    "    nn_model = Sequential([\n",
    "        #input layer\n",
    "        Dense(256, activation='relu', input_shape=(n_features,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        #Hidden Layer 1\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        #Hidden Layer 2\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        #Hidden Layer 3\n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        #output layer\n",
    "        Dense(n_classes, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "    #compile Model\n",
    "    nn_model.compile(\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss = 'categorical_crossentropy',\n",
    "        metrics = ['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "    )\n",
    "\n",
    "    print(\"Model Architecture:\")\n",
    "    nn_model.summary()\n",
    "\n",
    "    #callbacks\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        patience = 5,\n",
    "        restore_best_weights = True,\n",
    "        verbose = 1\n",
    "    )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor = 'val_loss',\n",
    "        factor = 0.5,\n",
    "        patience = 3,\n",
    "        min_lr = 0.00001,\n",
    "        verbose = 1\n",
    "    )\n",
    "\n",
    "    print(\"Training Neural Network:\")\n",
    "    print(f\" Early Stopping: Enabled (patience=5)\")\n",
    "    print(f\" Learning Rate Sheduling: Enabled\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot build the model , data not available.\")\n",
    "    nn_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if nn_model is not None and X_train_resampled is not None:\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    #train model\n",
    "    history = nn_model.fit(\n",
    "        X_train_resampled, y_train_cat,\n",
    "        epochs = 30,\n",
    "        batch_size = 512,\n",
    "        validation_split = 0.2,\n",
    "        callbacks = [early_stop, reduce_lr],\n",
    "        verbose = 1\n",
    "    )\n",
    "\n",
    "    train_time = time.time()-start_time\n",
    "\n",
    "    print(\"Training Complete for NN model:\")\n",
    "    print(f\" Training Time: {train_time:.2f} seconds\")\n",
    "    print(f\" Epochs Completed: {len(history.history['loss'])}\")\n",
    "\n",
    "    #predictions\n",
    "    y_pred_nn_proba = nn_model.predict(X_test_scaled, verbose=0)\n",
    "    y_pred_nn = np.argmax(y_pred_nn_proba, axis = 1)\n",
    "\n",
    "    #evaluation metrics\n",
    "    accuracy_nn = accuracy_score(y_test,y_pred_nn)\n",
    "    precision_nn = precision_score(y_test,y_pred_nn, average='weighted', zero_division=0)\n",
    "    recall_nn = recall_score(y_test, y_pred_nn, average='weighted', zero_division=0)\n",
    "    f1_nn = f1_score(y_test, y_pred_nn, average='weighted', zero_division = 0)\n",
    "\n",
    "    print(\"Neural Network Performance:\")\n",
    "    print(f\" Accuracy Score: {accuracy_nn:.4f}\")\n",
    "    print(f\" Precision Score: {precision_nn:.4f}\")\n",
    "    print(f\" Recall Score: {recall_nn:.4f}\")\n",
    "    print(f\" F1-score: {f1_nn:.4f}\")\n",
    "\n",
    "    #detailed classification report\n",
    "    print(\"Classification Report for neural network:\")\n",
    "    print(classification_report(\n",
    "        y_test, y_pred_nn,\n",
    "        target_names = le.classes_,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    #plot training history\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16,10))\n",
    "\n",
    "    #loss\n",
    "    ax1 = axes[0,0]\n",
    "    ax1.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    ax1.plot(history.history['val_loss'],label='Validation Loss', linewidth=2)\n",
    "    ax1.set_title('Model Loss:', fontsize=13, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch', fontsize = 11)\n",
    "    ax1.set_ylabel('Loss', fontsize = 11)\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "\n",
    "    #Acurracy\n",
    "    ax2 = axes[0,1]\n",
    "    ax2.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    ax2.set_title('Model Accuracy', fontsize=13, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch', fontsize=11)\n",
    "    ax2.set_ylabel('Accuracy', fontsize = 11)\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "\n",
    "    #Precision\n",
    "    ax3 = axes[1,0]\n",
    "    ax3.plot(history.history['precision'], label='Training Precision', linewidth=2)\n",
    "    ax3.plot(history.history['val_precision'],label='Validation Precision', linewidth=2)\n",
    "    ax3.set_title('Model Precision', fontsize=13, fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch', fontsize=11)\n",
    "    ax3.set_ylabel('Precision', fontsize=11)\n",
    "    ax3.legend()\n",
    "    ax3.grid(alpha=0.3)\n",
    "\n",
    "    #Recall\n",
    "    ax4 = axes[1,1]\n",
    "    ax4.plot(history.history['recall'], label='Training Recall', linewidth=2)\n",
    "    ax4.plot(history.history['val_recall'],label='Validation Recall',linewidth=2)\n",
    "    ax4.set_title('Model Recall', fontsize=13, fontweight='bold')\n",
    "    ax4.set_xlabel('Epoch', fontsize = 11)\n",
    "    ax4.set_ylabel('Recall', fontsize=11)\n",
    "    ax4.legend()\n",
    "    ax4.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Cannot train the model, not model or training data...\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutaion Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation - Security-Oriented Metrics\n",
    "\n",
    "In cybersecurity, **standard accuracy is not enough**. We need metrics that reflect:\n",
    "- **False Negatives (Missed Attacks)**: Most critical - these are undetected threats\n",
    "- **False Positives (False Alarms)**: Should be minimized but are less dangerous\n",
    "- **Per-Class Performance**: Some attacks are rarer but more dangerous\n",
    "\n",
    "### Key Security Metrics:\n",
    "1. **Confusion Matrix**: Understand misclassifications\n",
    "2. **Recall (Sensitivity)**: Ability to detect attacks - CRITICAL\n",
    "3. **Precision**: Accuracy of attack predictions\n",
    "4. **F1-Score**: Balance between precision and recall\n",
    "5. **ROC-AUC**: Overall discrimination capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare all models\n",
    "models_to_evaluate = {\n",
    "    'Random Forest' : (rf_model, y_pred_rf) if rf_model else None,\n",
    "    'XG Boost': (xgb_model, y_pred_xgb) if xgb_model else None,\n",
    "    'Neural Network': (nn_model, y_pred_nn) if nn_model else None\n",
    "}\n",
    "\n",
    "#filter none models\n",
    "models_to_evaluate = {k: v for k, v in models_to_evaluate.items() if v is not None}\n",
    "\n",
    "if models_to_evaluate:\n",
    "    n = len(models_to_evaluate)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(8*n,6))\n",
    "\n",
    "    if n==1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, (model_name, (model,y_pred)) in enumerate(models_to_evaluate.items()):\n",
    "\n",
    "        #confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        #normalise bt true labels(rows)\n",
    "        cm_normalized = cm.astype('float')/cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        #plot\n",
    "        ax = axes[idx]\n",
    "        im = ax.imshow(cm_normalized, interpolation = 'nearest', cmap = 'YlOrRd')\n",
    "        ax.set_title(f'{model_name}\\n Normalized Confusion Matrix', fontsize = 12, fontweight = 'bold')\n",
    "\n",
    "        #colorbar\n",
    "        plt.colorbar(im, ax=ax, fraction = 0.046, pad=0.04)\n",
    "\n",
    "        #labels\n",
    "        tick_marks = np.arange(len(le.classes_))\n",
    "        ax.set_xticks(tick_marks)\n",
    "        ax.set_yticks(tick_marks)\n",
    "\n",
    "        #showing class ids also\n",
    "        ax.set_xticklabels([f'C{i}' for i in range(len(le.classes_))],rotation=45)\n",
    "        ax.set_yticklabels([f'C{i}' for i in range(len(le.classes_))])\n",
    "\n",
    "        ax.set_xlabel('True Label', fontsize = 10)\n",
    "        ax.set_ylabel('Predicted Label', fontsize = 10)\n",
    "\n",
    "        #add text annotations for diagonal (correct predicitions)\n",
    "        for i in range(len(le.classes_)):\n",
    "            text_color = 'white' if cm_normalized[i, i]>0.5 else 'black'\n",
    "            ax.text(i, i, f'{cm_normalized[i,i]:.2f}', ha=\"center\", va=\"center\", color = text_color, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    best_model_name = max(models_to_evaluate.keys(),\n",
    "                          key = lambda k: f1_score(y_test, models_to_evaluate[k][1], average='weighted'))\n",
    "    best_pred = models_to_evaluate[best_model_name][1]\n",
    "\n",
    "    print(f\"\\n Detailed Confusion Matrix - {best_model_name}:\")\n",
    "    cm_best = confusion_matrix(y_test, best_pred)\n",
    "    cm_df = pd.DataFrame(cm_best,\n",
    "                        index = [f'{le.classes_[i]} (C{i})' for i in range(len(le.classes_))],\n",
    "                        columns = [f'Pred_C{i}' for i in range(len(le.classes_))])\n",
    "\n",
    "    print(cm_df)\n",
    "\n",
    "    print(f\" Common Misclassification:\")\n",
    "    misclass = []\n",
    "    for i in range(len(cm_best)):\n",
    "        for j in range(len(cm_best)):\n",
    "            if i!=j and cm_best[i,j]>0:\n",
    "                misclass.append((le.classes_[i], le.classes_[j], cm_best[i,j]))\n",
    "\n",
    "    #sort by count\n",
    "    misclass.sort(key=lambda x:x[2], reverse=True)\n",
    "\n",
    "    for true_class, pred_class, count in misclass[:10]:\n",
    "        print(f\" {true_class:30s}->{pred_class:30s}:{count:5,} samples\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per Class Security Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_to_evaluate:\n",
    "    best_model_name = max(models_to_evaluate.keys(),\n",
    "                         key = lambda k: f1_score(y_test, models_to_evaluate[k][1], average='weighted'))\n",
    "    best_pred = models_to_evaluate[best_model_name][1]\n",
    "\n",
    "    print(f\"\\n Analysis based on {best_model_name}:\")\n",
    "\n",
    "    #calculate per class metrics\n",
    "    report = classification_report(y_test, best_pred,\n",
    "                                  target_names = le.classes_,\n",
    "                                  output_dict = True,\n",
    "                                  zero_division = 0)\n",
    "\n",
    "    #dataframe for visualisation\n",
    "    class_metrics = []\n",
    "    for class_name in le.classes_:\n",
    "        if class_name in report:\n",
    "            class_metrics.append({\n",
    "                'Class' : class_name,\n",
    "                'Precision' : report[class_name]['precision'],\n",
    "                'Recall' : report[class_name]['recall'],\n",
    "                'F1-Score': report[class_name]['f1-score'],\n",
    "                'Support': report[class_name]['support']\n",
    "            })\n",
    "\n",
    "    metrics_df = pd.DataFrame(class_metrics)\n",
    "\n",
    "    #sorting by recall important for security\n",
    "    metrics_df_sorted = metrics_df.sort_values('Recall', ascending=True)\n",
    "\n",
    "    #visualisation\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18,12))\n",
    "\n",
    "    #Recall by class\n",
    "    ax1 = axes[0,0]\n",
    "    colors_recall = ['red' if r<0.7 else 'orange' if r<0.9 else 'green'\n",
    "                    for r in metrics_df_sorted['Recall']]\n",
    "    ax1.barh(metrics_df_sorted['Class'], metrics_df_sorted['Recall'],\n",
    "            color = colors_recall, edgecolor = 'black')\n",
    "    ax1.set_xlabel('Recall Score', fontsize=11)\n",
    "    ax1.set_title('Recall by Attack Type (Attack Detection Rate)', fontsize = 12, fontweight='bold')\n",
    "    ax1.axvline(x=0.9, color='green', linestyle='--', linewidth=2, alpha=0.5, label='Excellent (>0.9)')\n",
    "    ax1.axvline(x=0.7, color='orange', linestyle='--', linewidth=2, alpha=0.5, label='Good (>0.7)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    #precision by class\n",
    "    ax2 = axes[0,1]\n",
    "    metrics_df_sorted_prec = metrics_df.sort_values('Precision', ascending=True)\n",
    "    ax2.barh(metrics_df_sorted_prec['Class'], metrics_df_sorted_prec['Precision'],\n",
    "            color = 'skyblue', edgecolor='black')\n",
    "    ax2.set_xlabel('Precision Score', fontsize = 11)\n",
    "    ax2.set_title(f\"Precision by Attack Type (Prediction Accuracy)\", fontsize=12, fontweight = 'bold')\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    #F-1 score by class\n",
    "    ax3 = axes[1,0]\n",
    "    metrics_df_sorted_f1 = metrics_df.sort_values('F1-Score', ascending=True)\n",
    "    ax3.barh(metrics_df_sorted_f1['Class'], metrics_df_sorted_f1['F1-Score'],\n",
    "            color='lightgreen', edgecolor='black')\n",
    "    ax3.set_xlabel('F1-Score', fontsize=11)\n",
    "    ax3.set_title(f\"F1-Score by Attack Type (Balanced Performance)\", fontsize=12, fontweight ='bold')\n",
    "    ax3.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    #Support Sample Count\n",
    "    ax4 = axes[1,1]\n",
    "    metrics_df_sorted_supp= metrics_df.sort_values('Support', ascending=True)\n",
    "    ax4.barh(metrics_df_sorted_supp['Class'], metrics_df_sorted_supp['Support'],\n",
    "            color='coral', edgecolor='black')\n",
    "    ax4.set_xlabel('Number of Samples', fontsize=11)\n",
    "    ax4.set_title(f'Test Set Distribution by Attack Type', fontsize=12, fontweight='bold')\n",
    "    ax4.set_xscale('log')\n",
    "    ax4.grid(axis='x',alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n\\n Security Critical Insights:\")\n",
    "    print(f\"\\n Low Recall Classes (High Risk - Missed Attacks)\")\n",
    "    low_recall = metrics_df[metrics_df['Recall'] < 0.8].sort_values('Recall')\n",
    "    if len(low_recall)>0:\n",
    "        for _, row in low_recall.iterrows():\n",
    "            print(f\" {row['Class']:30s}: Recall={row['Recall']:.3f} (Missing {(1-row['Recall'])*100:.1f}% of attacks.)\")\n",
    "    else:\n",
    "        print(f\"All classes have recall >0.8\")\n",
    "\n",
    "    print(f\"\\n Low Precision Classes (False Alarm Risks)\")\n",
    "    low_precision = metrics_df[metrics_df['Precision'] < 0.8].sort_values('Precision')\n",
    "    if len(low_precision)>0:\n",
    "        for _, row in low_precision.iterrows():\n",
    "            print(f\" {row['Class']:30s}: Precision={row['Precision']:.3f} ({(1-row['Precision'])*100:.1f}% false positives.)\")\n",
    "    else:\n",
    "        print(\"All classes have precision >0.8\")\n",
    "else:\n",
    "    print(\"No models to evaluate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve Analysis (Binary Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary Classification Benign vs Malicious\n",
    "if X_test_scaled is not None:\n",
    "    print(\"\\n Computing Binary Predictions (Benign vs Malicious)\")\n",
    "\n",
    "    #binary target\n",
    "    y_test_binary_val = (y_test != 0).astype(int) #assuming Benign as 0\n",
    "\n",
    "    #get predictions from all models\n",
    "    binary_roc_data = {}\n",
    "\n",
    "    if rf_model:\n",
    "        #get probability of being malicious any class except Benign\n",
    "        y_pred_rf_proba = rf_model.predict_proba(X_test_scaled)\n",
    "        y_pred_rf_binary = 1 - y_pred_rf_proba.values[:,0]\n",
    "        binary_roc_data['Random Forest'] = y_pred_rf_binary\n",
    "\n",
    "\n",
    "    if xgb_model:\n",
    "        y_pred_xgb_binary = 1 - xgb_model.predict_proba(X_test_scaled)[:,0] \n",
    "        binary_roc_data['XGBoost'] = y_pred_xgb_binary\n",
    "\n",
    "    if nn_model:\n",
    "        y_pred_nn_binary = 1 - y_pred_nn_proba[:,0]\n",
    "        binary_roc_data['Neural Network'] = y_pred_nn_binary\n",
    "\n",
    "\n",
    "    #plot ROC Curves\n",
    "    plt.figure(figsize=(10,8))\n",
    "\n",
    "    for model_name, y_pred_proba_binary in binary_roc_data.items():\n",
    "        fpr, tpr , thresholds = roc_curve(y_test_binary_val, y_pred_proba_binary)\n",
    "        roc_auc = roc_auc_score(y_test_binary_val, y_pred_proba_binary)\n",
    "\n",
    "        plt.plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC = {roc_auc:.4f})')\n",
    "\n",
    "        print(f\"{model_name}:\")\n",
    "        print(f\" ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "    #plot diagonal (random classifier)\n",
    "    plt.plot([0,1],[0,1], 'k--', linewidth=2, label='Random Classifier (AUC = 0.5000)')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rates (False Alarms)', fontsize=12)\n",
    "    plt.ylabel('True Positive Rates (Attack Detection)', fontsize=12)\n",
    "\n",
    "    plt.title('ROC Curve Binary Classification: (Benign vs Malicious)', fontsize = 14, fontweight = 'bold')\n",
    "    plt.legend(loc=\"lower right\", fontsize=10)\n",
    "    plt.grid(alpha = 0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n Interpretation:\")\n",
    "    print(f\" AUC close to 1.0 = Extreme Discrimination\")\n",
    "    print(f\" AUC around 0.5 = No better than random\")\n",
    "    print(f\" Higher TPR at Low FPR = Better Security Performance\")\n",
    "\n",
    "else:\n",
    "    print(\"ROC analysis can not be done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all models comparison\n",
    "all_models_results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost', 'LightGBM', 'Neural Network'],\n",
    "    'Accuracy': [accuracy_lr, accuracy_rf, accuracy_xgb, accuracy_lgb, accuracy_nn],\n",
    "    'Precision': [precision_lr, precision_rf, precision_xgb, precision_lgb, precision_nn],\n",
    "    'Recall': [recall_lr, recall_rf, recall_xgb, recall_lgb, recall_nn],\n",
    "    'F1-Score': [f1_lr, f1_rf, f1_xgb, f1_lgb, f1_nn]\n",
    "})\n",
    "\n",
    "print(\"\\n Complete Performance Summary:\")\n",
    "print(all_models_results.to_string(index=False))\n",
    "\n",
    "#finding best performer\n",
    "print(\"\\n Best Model by metric:\")\n",
    "for metric in ['Accuracy','Precision','Recall','F1-Score']:\n",
    "    best_idx = all_models_results[metric].idxmax()\n",
    "    best_model = all_models_results.loc[best_idx, 'Model']\n",
    "    best_score = all_models_results.loc[best_idx, metric]\n",
    "    print(f\" {metric:10s}: {best_model:20s} ({best_score:.4f})\")\n",
    "\n",
    "#visualisation Radar Chart\n",
    "fig, ax = plt.subplots(figsize=(10,10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "N = len(categories)\n",
    "\n",
    "angles = [n/float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "ax.set_theta_offset(np.pi/2)\n",
    "ax.set_theta_direction(-1)\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, size=12)\n",
    "\n",
    "colors = plt.cm.tab10(range(len(all_models_results)))\n",
    "\n",
    "for idx, row in all_models_results.iterrows():\n",
    "    values = row[categories].values.flatten().tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'], color=colors[idx])\n",
    "    ax.fill(angles, values, alpha=0.15, color = colors[idx])\n",
    "\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], size=10)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
    "plt.title('Model Perfomance Radar Chart', size=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Explainability & Feature Importance\n",
    "\n",
    "**Why Explainability Matters in Cybersecurity:**\n",
    "- Security analysts need to understand WHY a system flagged traffic as malicious\n",
    "- Regulatory compliance (GDPR, CCPA) requires explainable AI decisions\n",
    "- Debug false positives and improve detection rules\n",
    "- Gain insights into attack patterns\n",
    "\n",
    "### Techniques:\n",
    "1. **Feature Importance** (Tree-based models)\n",
    "2. **SHAP Values** (SHapley Additive exPlanations) - Gold standard for explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def get_feature_importance(model, X=None, y=None, model_name=\"Model\"):\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        return model.feature_importances_\n",
    "\n",
    "    if hasattr(model, \"feature_importance\"):\n",
    "        return model.feature_importance()\n",
    "\n",
    "    if X is not None and y is not None:\n",
    "        print(f\" Using permutation importance for {model_name}:\")\n",
    "        result = permutation_importance(model, X, y, n_repeats=10, random_state=42)\n",
    "        return result.importances_mean\n",
    "\n",
    "    raise AttributeError(f\" Feature Importance not available for {model_name} without X and y.\")\n",
    "#get feature importance based on tree models\n",
    "feature_importance_data = {}\n",
    "\n",
    "if rf_model:\n",
    "    feature_importance_data['Random Forest'] = get_feature_importance(rf_model, X_test_scaled, y_test, \"Random Forest\")\n",
    "\n",
    "if xgb_model:\n",
    "    feature_importance_data['XGBoost'] = get_feature_importance(xgb_model, X_test_scaled, y_test, \"XGBoost\")\n",
    "\n",
    "if lgb_model:\n",
    "    feature_importance_data['LightGBM'] = get_feature_importance(lgb_model, X_test_scaled, y_test, \"LightGBM\")\n",
    "\n",
    "if feature_importance_data:\n",
    "    #create dataframe\n",
    "    importance_df = pd.DataFrame(feature_importance_data, index=feature_names)\n",
    "\n",
    "    #calculate average importance\n",
    "    importance_df['Average'] = importance_df.mean(axis=1)\n",
    "    importance_df_sorted = importance_df.sort_values('Average', ascending=False)\n",
    "\n",
    "    print(f\"Top 20 Important Features:\\n\")\n",
    "    print(importance_df_sorted.head(20))\n",
    "\n",
    "    #visualisation\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18,12))\n",
    "\n",
    "    top_n =15\n",
    "\n",
    "    #plot for each model\n",
    "    for idx, (model_name, ax) in enumerate(zip(feature_importance_data.keys(), axes.flat[:len(feature_importance_data)])):\n",
    "        top_features = importance_df.nlargest(top_n, model_name)[model_name]\n",
    "\n",
    "        ax.barh(range(len(top_features)), top_features.values, color='teal', edgecolor='black')\n",
    "        ax.set_yticks(range(len(top_features)))\n",
    "        ax.set_yticklabels(top_features.index, fontsize=9)\n",
    "        ax.set_xlabel('Importance Score', fontsize=11)\n",
    "        ax.set_title(f\" Top {top_n} Features-{model_name}\", fontsize=12, fontweight='bold')\n",
    "        ax.invert_yaxis()\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    #average importanve across all models\n",
    "    if len(feature_importance_data)>1:\n",
    "        ax_avg = axes.flat[len(feature_importance_data)]\n",
    "        top_avg = importance_df_sorted['Average'].head(top_n)\n",
    "\n",
    "        ax_avg.barh(range(len(top_avg)), top_avg.values, color='coral', edgecolor='black')\n",
    "        ax_avg.set_yticks(range(len(top_avg)))\n",
    "        ax_avg.set_yticklabels(top_avg.index, fontsize=9)\n",
    "        ax_avg.set_xlabel('Average Importance Score', fontsize=11)\n",
    "        ax_avg.set_title(f\" Top {top_n} Features - Average Across all models\", fontsize=12, fontweight='bold')\n",
    "        ax_avg.invert_yaxis()\n",
    "        ax_avg.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #security insights\n",
    "    print(f\" Security Insights from Feature Importance:\")\n",
    "    print(f\" \\n Top 5 Attack Indicators:\")\n",
    "    for idx,(feature, importance) in enumerate(importance_df_sorted['Average'].head(5).items(), 1):\n",
    "        print(f\" {idx}. {feature:40s}: {importance:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(f\"No Tree based models available for Feature Importance Analysis.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shapely Additive Explaination\n",
    "import shap\n",
    "#choose best tree model for SHAP analysis\n",
    "if xgb_model:\n",
    "    chosen_model = xgb_model\n",
    "    model_name = \"XGBoost\"\n",
    "elif lgb_model:\n",
    "    chosen_model = lgb_model\n",
    "    model_name = \"LightGBM\"\n",
    "elif rf_model:\n",
    "    chosen_model = rf_model\n",
    "    model_name = \"Random Forest\"\n",
    "else:\n",
    "    chosen_model = None\n",
    "    model_name = None\n",
    "\n",
    "if chosen_model:\n",
    "    print(f\" Computing SHAP values for the {model_name}:\")\n",
    "\n",
    "    #sample subset for Shap to save computation Time\n",
    "    sample_size = min(500, len(X_test_scaled))\n",
    "    X_test_sample = X_test_scaled.sample(n=sample_size, random_state=42)\n",
    "\n",
    "    #create a SHAP explainer:\n",
    "    # if model_name == \"XGBoost\":\n",
    "    #     explainer = shap.TreeExplainer(chosen_model)\n",
    "    # else:\n",
    "    #     explainer = shap.TreeExplainer(chosen_model)\n",
    "\n",
    "    # explainer = shap.Explainer(chosen_model, X_test_sample)\n",
    "    # shap_values = explainer(X_test_sample)\n",
    "    # shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "    if model_name == \"XGBoost\":\n",
    "        explainer = shap.Explainer(chosen_model.predict_proba, X_test_sample)\n",
    "        shap_values = explainer(X_test_sample)\n",
    "    elif model_name == \"LightGBM\":\n",
    "        shap_values = chosen_model.predict(X_test_sample, pred_contrib=True)\n",
    "    elif model_name== \"Random Forest\":\n",
    "        explainer = shap.TreeExplainer(chosen_model)\n",
    "        shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "    print(f\" SHAP values Computed.\")\n",
    "\n",
    "    #SHAP summary PLOT (Global Feature Importance)\n",
    "    print(f\"Generating SHAP summary Plot.\")\n",
    "    plt.figure(figsize=(12,8))\n",
    "\n",
    "    #For multi-class use first class or average\n",
    "    if isinstance(shap_values.values, list):\n",
    "        shap_values_plot = shap_values.values[1] #use malicious class\n",
    "    else:\n",
    "        shap_values_plot = shap_values.values\n",
    "\n",
    "    shap.summary_plot(shap_values_plot, X_test_sample, feature_names=feature_names,\n",
    "                     show=False, max_display=20)\n",
    "    plt.title(f\"SHAP summary plot - {model_name}\\n (Global Feature Impact)\", fontsize = 14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #SHAP Feature Importance (Mean Absolute)\n",
    "    print(f\"Generating SHAP Feature Importance:\")\n",
    "    plt.figure(figsize=(12,8))\n",
    "    shap.summary_plot(shap_values_plot, X_test_sample,\n",
    "                     feature_names=feature_names,\n",
    "                     plot_type=\"bar\", show=False, max_display=20)\n",
    "    plt.title(f\" SHAP Feature Importance - {model_name}\\n (MEAN | SHAP VALUE)\", fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n SHAP INTERPRETATION:\")\n",
    "    print(f\" Red points : High Feature Values\")\n",
    "    print(f\" Blue Points: Low Feature Values\")\n",
    "    print(f\" Position on X-axis = Impact on model output.\")\n",
    "    print(f\" Features at top= Most Important for predictions.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n No model available for SHAP analysis.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Prediction Explaination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if chosen_model and 'shap_values' in locals():\n",
    "    #select a few interesting samples\n",
    "    print(f\"Analysing sample Predictions.\")\n",
    "\n",
    "    #find out correctly classified malicious sample\n",
    "    malicious_indices = np.where((y_test!=0) & (y_pred_xgb!=0) if xgb_model else (y_test!=0))[0]\n",
    "\n",
    "    if len(malicious_indices)>0:\n",
    "        sample_idx = np.random.choice(malicious_indices)\n",
    "        sample_data = X_test_scaled.iloc[sample_idx: sample_idx+1]\n",
    "\n",
    "        true_label = le.inverse_transform([y_test[sample_idx]])[0]\n",
    "        pred_label = le.inverse_transform([y_pred_xgb[sample_idx]])[0] if xgb_model else \"N/A\"\n",
    "\n",
    "        print(\"Sample Analysis:\\n\")\n",
    "        print(f\" True label: {true_label}\")\n",
    "        print(f\" Predicted: {pred_label}\")\n",
    "\n",
    "        #shap force plot\n",
    "        print(\"\\n Generating SHAP Force Plot:\")\n",
    "\n",
    "        #get shap values for this sample\n",
    "        sample_shap = explainer.shap_values(sample_data)\n",
    "        if isinstance(sample_shap, list):\n",
    "            sample_shap_plot = sample_shap[1][0] #malicoius class\n",
    "        else:\n",
    "            sample_shap_plot = sample_shap[0]\n",
    "\n",
    "        baseline = np.mean(chosen_model.predict_proba(X_test_sample), axis=0)\n",
    "\n",
    "        if baseline.ndim>0:\n",
    "            expected_value = baseline[1] if len(baseline)>1 else baseline[0]\n",
    "        else:\n",
    "            expected_value = baseline\n",
    "\n",
    "        \n",
    "\n",
    "        sample_shap_plot = np.array(sample_shap_plot).flatten()\n",
    "        sample_features = np.array(sample_data.iloc[0].values).flatten()\n",
    "\n",
    "        print(\"len(shap_values):\", len(sample_shap_plot))\n",
    "        print(\"len(features):\", len(sample_data.iloc[0].values))\n",
    "        print(\"len(feature_names):\", len(feature_names))\n",
    "        #create force plot\n",
    "        shap.force_plot(\n",
    "            # explainer.expected_value if not isinstance(explainer.expected_value, list) else explainer.expected_value[1],\n",
    "            expected_value,\n",
    "            sample_shap_plot.flatten(), #ensure 1D\n",
    "            sample_data.iloc[0].values.flatten(),\n",
    "            feature_names = feature_names,\n",
    "            matplotlib = True,\n",
    "            show = False\n",
    "        )\n",
    "        plt.title(f\" SHAP Force plot: Indvidual Prediction Explaination\\n {true_label}->{pred_label}\", fontsize = 12, fontweight='bold', pad=15)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        #show top contributing features\n",
    "        feature_contrib = pd.DataFrame({\n",
    "            'Feature' : feature_names,\n",
    "            'SHAP Value': sample_shap_plot,\n",
    "            'Feature Value': sample_data.iloc[0].values\n",
    "        }).sort_values('SHAP Value', key=abs, ascending=False)\n",
    "\n",
    "        print(f\"\\n TOP 10 contirbuting features in this prediction:\")\n",
    "        print(feature_contrib.head(10).to_string(index=False))\n",
    "\n",
    "        print(\"\\n Interpretation:\")\n",
    "        print(f\" -> Positive SHAP Values push prediction towards 'Malicious'\")\n",
    "        print(f\" -> Negative SHAP Values push prediction towards 'Benign'\")\n",
    "        print(f\" -> Magnitude: strength of contribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "#create directory for models\n",
    "os.makedirs('models', exist_ok = True)\n",
    "\n",
    "print(\"Saving models to ./models/ directory:\")\n",
    "\n",
    "#save models\n",
    "\n",
    "saved_models = []\n",
    "\n",
    "if rf_model:\n",
    "    joblib.dump(rf_model, 'models/random_forest.pkl')\n",
    "    saved_models.append('Random Forest')\n",
    "    print(\" Saved: random_forest.pkl\")\n",
    "\n",
    "if xgb_model:\n",
    "    joblib.dump(xgb_model, 'models/xgboost_ids.pkl')\n",
    "    saved_models.append('XGBoost')\n",
    "    print(\" Saved: xgboost_ids.pkl\")\n",
    "\n",
    "if lgb_model:\n",
    "    joblib.dump(lgb_model, 'models/lightgbm_ids.pkl')\n",
    "    saved_models.append('LightGBM')\n",
    "    print(\" Saved: lightgbm_ids.pkl\")\n",
    "\n",
    "if nn_model:\n",
    "    nn_model.save('models/neural_network_ids.h5')\n",
    "    saved_models.append('Neural Network')\n",
    "    print(\" Saved: neural_network_ids.h5\")\n",
    "\n",
    "#save preprocessing objects\n",
    "joblib.dump(scaler, 'models/scaler.pkl')\n",
    "joblib.dump(le, 'models/label_encoder.pkl')\n",
    "print(\"Saved: scaler.pkl\")\n",
    "print(\"Saved: label_encoder.pkl\")\n",
    "\n",
    "print(f\"\\n Succesfully Saved {len(saved_models)} models!\")\n",
    "print(f\"\\n Saved Models: {', '.join(saved_models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"/n Dataset Summary:\")\n",
    "print(f\" Total Samples: {len(df):,}\")\n",
    "print(f\" Features: {len(feature_names)}\")\n",
    "print(f\" Attack Types: {len(le.classes_)}\")\n",
    "print(f\" Training Samples: {len(y_train):,}\")\n",
    "print(f\" Test Samples: {len(y_test):,}\")\n",
    "\n",
    "print(\"\\n Models Trained:\")\n",
    "models_list = [\"Logistic Regression\", \"Random Forest\", \"XGBoost\", \"LightGBM\", \"Neural Network\"]\n",
    "for i,model in enumerate(models_list,1):\n",
    "    print(f\" {i}. {model}\")\n",
    "\n",
    "print(f\"\\n Best Performing Model.\")\n",
    "best_f1_idx = all_models_results['F1-Score'].idxmax()\n",
    "best_model_final = all_models_results.loc[best_f1_idx, 'Model']\n",
    "best_f1_final = all_models_results.loc[best_f1_idx, 'F1-Score']\n",
    "best_recall_final = all_models_results.loc[best_f1_idx, 'Recall']\n",
    "\n",
    "print(f\" Model: {best_model_final}\")\n",
    "print(f\" F1-Score: {best_f1_final:.4f}\")\n",
    "print(f\" Recall(Attack Detection): {best_recall_final:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3674161,
     "sourceId": 6376134,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
